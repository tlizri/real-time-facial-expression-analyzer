{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNLeReB8sjov"
      },
      "source": [
        "from tensorflow.config import list_physical_devices\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from numpy import arange\n",
        "from numpy.random import shuffle\n",
        "from math import ceil\n",
        "from cv2 import imread, resize, IMREAD_COLOR\n",
        "from numpy import array, load, sum\n",
        "from keras import Model\n",
        "from tensorflow.keras.applications import VGG16, EfficientNetB0, Xception, MobileNetV2\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgA_LjP5ssNJ",
        "outputId": "fc998226-917b-46ac-f6c3-721f9454d715"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(list_physical_devices('GPU')))\n",
        "\n",
        "CLASSES = 3\n",
        "BATCH_SIZE = 32\n",
        "RED = 'efficientnet'    # vgg16, efficientnet, xception\n",
        "pchs = 9\n",
        "lr=0.001\n",
        "if RED == 'vgg16':\n",
        "    MIN_SIZE = 224\n",
        "elif RED == 'efficientnet':\n",
        "    MIN_SIZE = 224\n",
        "elif RED == 'xception':\n",
        "    MIN_SIZE = 299\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "PATH = '/gdrive/My Drive/AffectNet'\n",
        "\n",
        "FILTRAR = 'filt'             # string -> filt, nofilt\n",
        "HOMO = 'balan'                # string -> balan, nobalan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k87z8tDjstFp"
      },
      "source": [
        "class AffectNetSequence(Sequence):\n",
        "    # Clase para realizar una carga de imagenes por lotes debido a la magnitud\n",
        "    # del dataset\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = arange(len(self.x))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.x) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        index = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[index]\n",
        "        batch_y = self.y[index]\n",
        "\n",
        "        return array([resize(imread(file_name, IMREAD_COLOR),\n",
        "                                (MIN_SIZE,MIN_SIZE))/255.0\n",
        "                                for file_name in batch_x], dtype=float), \\\n",
        "               array(batch_y, dtype = int)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        shuffle(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "3cZJ_mNKswOZ",
        "outputId": "d071bcd5-7ec9-4992-ae6e-23c4988fa8e6"
      },
      "source": [
        "x_train = load(PATH + '/x_train_data_' + str(CLASSES) + 'classes_' + FILTRAR + '_' + HOMO + '_' + 'size=' + str(MIN_SIZE) + '.npy', allow_pickle=True)\n",
        "x_val = load(PATH + '/x_val_data_' + str(CLASSES) + 'classes_' + FILTRAR + '_' + HOMO + '_' + 'size=' + str(MIN_SIZE) + '.npy', allow_pickle=True)\n",
        "y_train = load(PATH + '/y_train_data_' + str(CLASSES) + 'classes_' + FILTRAR + '_' + HOMO + '_' + 'size=' + str(MIN_SIZE) + '.npy', allow_pickle=True)\n",
        "y_val = load(PATH + '/y_val_data_' + str(CLASSES) + 'classes_' + FILTRAR + '_' + HOMO + '_' + 'size=' + str(MIN_SIZE) + '.npy', allow_pickle=True)\n",
        "y_t = sum(y_train, axis=0)\n",
        "y_v = sum(y_val, axis=0)\n",
        "print(\"Etiquetas de entrenamiento: \" + str(y_t))\n",
        "print(\"Etiquetas de evaluacion: \" + str(y_v))\n",
        "print(\"Imagenes de entrenamiento: \" + str(len(x_train)))\n",
        "print(\"Imagenes de validación: \" + str(len(x_val)))\n",
        "print('Ratio train-val: ' + str(int(100*((len(x_val))/((len(x_train))+(len(x_val)))))) + '%')\n",
        "\n",
        "train_generator = AffectNetSequence(x_train, y_train, BATCH_SIZE)\n",
        "val_generator = AffectNetSequence(x_val, y_val, BATCH_SIZE)\n",
        "\n",
        "if RED == 'vgg16':\n",
        "    model = VGG16(\n",
        "                  include_top=True,\n",
        "                  weights=None,\n",
        "                  classes=CLASSES,\n",
        "                  classifier_activation=\"softmax\")\n",
        "\n",
        "elif RED == 'efficientnet':\n",
        "    model = EfficientNetB0(\n",
        "                            include_top=True,\n",
        "                            weights=None,\n",
        "                            classes=CLASSES,\n",
        "                            classifier_activation=\"softmax\")\n",
        "    \n",
        "elif RED == 'xception':\n",
        "    model = Xception(\n",
        "                    include_top=True,\n",
        "                    weights=None,\n",
        "                    classes=CLASSES,\n",
        "                    classifier_activation=\"softmax\")\n",
        "elif RED == 'mobilenetv2':\n",
        "    model = MobileNetV2(\n",
        "                        include_top=True,\n",
        "                        weights=None,\n",
        "                        classes=CLASSES,\n",
        "                        classifier_activation=\"softmax\")\n",
        "    \n",
        "model.compile(optimizer=Adam(learning_rate=lr,\n",
        "                              beta_1=0.9,\n",
        "                              beta_2=0.999,\n",
        "                              epsilon=1e-07),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "start_time = time()\n",
        "history = model.fit(\n",
        "    x=train_generator,\n",
        "    epochs=pchs,\n",
        "    verbose=1,\n",
        "    validation_data=val_generator,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=(ceil(len(x_train) / BATCH_SIZE)),\n",
        "    max_queue_size=8,\n",
        "    workers=4,\n",
        "    use_multiprocessing=True\n",
        "    )\n",
        "end_time = time()\n",
        "execution_time = round((end_time - start_time)/3600, 2)\n",
        "print(str(execution_time))\n",
        "\n",
        "model.save(PATH + '/' + RED + '_learningrate=' + str(lr) + '_' + str(CLASSES) + 'classes_epoch=' + str(pchs) + '_time'+ str(execution_time) + '_' + FILTRAR + '_' + HOMO + '.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etiquetas de entrenamiento: [34386 34406 34408]\n",
            "Etiquetas de evaluacion: [8614 8594 8592]\n",
            "Imagenes de entrenamiento: 103200\n",
            "Imagenes de validación: 25800\n",
            "Ratio train-val: 20%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8ffa67c35700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    926\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
            "\u001b[0;32m<ipython-input-3-ad8db8bc3336>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m         return array([resize(imread(file_name, IMREAD_COLOR),\n\u001b[1;32m     19\u001b[0m                                 (MIN_SIZE,MIN_SIZE))/255.0\n\u001b[0;32m---> 20\u001b[0;31m                                 for file_name in batch_x], dtype=float), \\\n\u001b[0m\u001b[1;32m     21\u001b[0m                \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ad8db8bc3336>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m         return array([resize(imread(file_name, IMREAD_COLOR),\n\u001b[1;32m     19\u001b[0m                                 (MIN_SIZE,MIN_SIZE))/255.0\n\u001b[0;32m---> 20\u001b[0;31m                                 for file_name in batch_x], dtype=float), \\\n\u001b[0m\u001b[1;32m     21\u001b[0m                \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnh5KFVrTo79"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'bo', label='Entrenamiento')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validación')\n",
        "plt.title('Precisión de entrenamiento y validación')\n",
        "plt.ylabel(\"Pasadas\")\n",
        "plt.xlabel(\"Precisión\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Entrenamiento')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validación')\n",
        "plt.title('Perdidas de entrenamiento y validación')\n",
        "plt.ylabel(\"Pasadas\")\n",
        "plt.xlabel(\"Pérdidas\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}